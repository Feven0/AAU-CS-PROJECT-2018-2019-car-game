{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWU7bFHxTVxsaysVVeyMEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feven0/AAU-CS-PROJECT-2018-2019-car-game/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br8wH6CSOJ9R",
        "outputId": "838659b0-2b5d-41cc-fce2-b06c2dcb975b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at uhhlt/am-roberta and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Batch [49], Loss: 10.2450\n",
            "Epoch [1/50], Batch [99], Loss: 10.2266\n",
            "Epoch [1/50], Batch [149], Loss: 10.2114\n",
            "Epoch [1/50], Batch [199], Loss: 10.1949\n",
            "Epoch [1/50], Batch [249], Loss: 10.1778\n",
            "Epoch [1/50], Batch [299], Loss: 10.1619\n",
            "Epoch [1/50], Batch [349], Loss: 10.1452\n",
            "Epoch [1/50], Batch [399], Loss: 10.1269\n",
            "Epoch [1/50], Batch [449], Loss: 10.1126\n",
            "Epoch [1/50], Batch [499], Loss: 10.0977\n",
            "Epoch [1/50], Batch [549], Loss: 10.0804\n",
            "Epoch [1/50], Batch [599], Loss: 10.0523\n",
            "Epoch [1/50], Batch [649], Loss: 10.0475\n",
            "Epoch [1/50], Batch [699], Loss: 10.0255\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM, RobertaTokenizer, RobertaForTokenClassification\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"uhhlt/am-roberta\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"uhhlt/am-roberta\")\n",
        "\n",
        "# Read the tagged data\n",
        "tagged_data_path = r'/content/tagged_modified_segments.txt'\n",
        "with codecs.open(tagged_data_path, 'r', encoding='utf-8') as file:\n",
        "    tagged_data = file.readlines()\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "num_examples = len(tagged_data)\n",
        "train_size = int(num_examples * train_ratio)\n",
        "val_size = int(num_examples * val_ratio)\n",
        "\n",
        "train_data = tagged_data[:train_size]\n",
        "val_data = tagged_data[train_size:train_size + val_size]\n",
        "test_data = tagged_data[train_size + val_size:]\n",
        "\n",
        "train_file = r'/content/train.txt'\n",
        "val_file = r'/content/validation.txt'\n",
        "test_file = r'/content/test.txt'\n",
        "\n",
        "with codecs.open(train_file, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(train_data)\n",
        "\n",
        "with codecs.open(val_file, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(val_data)\n",
        "\n",
        "with codecs.open(test_file, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(test_data)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(file_path, exclude_labels=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    unseen_labels = set()\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            if line != '\\n':\n",
        "                tokens = line.strip().split()\n",
        "                text = tokens[0]\n",
        "                if len(tokens) > 1:\n",
        "                    label = tokens[1]\n",
        "                else:\n",
        "                    label = 'unknown'  # Use a default label if no label is provided\n",
        "                if exclude_labels is None or label not in exclude_labels:\n",
        "                    texts.append(text)\n",
        "                    labels.append(label)\n",
        "                else:\n",
        "                    unseen_labels.add(label)\n",
        "    return texts, labels, unseen_labels\n",
        "\n",
        "train_texts, train_labels, train_unseen_labels = preprocess_data(train_file)\n",
        "val_texts, val_labels, _ = preprocess_data(val_file, exclude_labels=train_unseen_labels)\n",
        "test_texts, test_labels, _ = preprocess_data(test_file, exclude_labels=train_unseen_labels)\n",
        "\n",
        "# Encode the labels\n",
        "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "train_labels_reshaped = [[label] for label in train_labels]\n",
        "train_labels_encoded = oe.fit_transform(train_labels_reshaped)\n",
        "\n",
        "val_labels_reshaped = [[label] for label in val_labels]\n",
        "test_labels_reshaped = [[label] for label in test_labels]\n",
        "val_labels_encoded = oe.transform(val_labels_reshaped)\n",
        "test_labels_encoded = oe.transform(test_labels_reshaped)\n",
        "\n",
        "test_labels_encoded[test_labels_encoded == oe.categories_[0].size] = -1  # Replace unknown categories with -1\n",
        "\n",
        "train_labels = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_labels_encoded, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
        "\n",
        "# Tokenize the texts\n",
        "train_encoded_inputs = tokenizer(train_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "val_encoded_inputs = tokenizer(val_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "test_encoded_inputs = tokenizer(test_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_input_ids = train_encoded_inputs['input_ids']\n",
        "train_attention_mask = train_encoded_inputs['attention_mask']\n",
        "val_input_ids = val_encoded_inputs['input_ids']\n",
        "val_attention_mask = val_encoded_inputs['attention_mask']\n",
        "test_input_ids = test_encoded_inputs['input_ids']\n",
        "test_attention_mask = test_encoded_inputs['attention_mask']\n",
        "\n",
        "# TensorFlow setup\n",
        "os.environ['XLA_PYTHON_CLIENT_NUMA_AWARE'] = '1'\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = '0'\n",
        "\n",
        "# Prepare datasets for DataLoader\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(set(list(labels) + ['<PAD>']))}\n",
        "        self.num_labels = len(self.label_to_idx)\n",
        "        self.unknown_label_idx = self.label_to_idx['<PAD>']  # Index for unknown labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data[index]\n",
        "        label = self.labels[index] if index < len(self.labels) else 'unknown'  # Use 'unknown' if no label is available\n",
        "\n",
        "        encoded_input = tokenizer.encode_plus(text, padding='max_length', max_length=50, return_tensors='pt')\n",
        "\n",
        "        # Convert the label to a tensor\n",
        "        if isinstance(label, (list, tuple)):\n",
        "            # If the label is a sequence, convert it to a tensor\n",
        "            encoded_label = torch.tensor([self.label_to_idx.get(l, self.unknown_label_idx) for l in label])\n",
        "        else:\n",
        "            # If the label is a single value, convert it to a tensor with the same sequence length\n",
        "            encoded_label = torch.tensor([self.label_to_idx.get(label, self.unknown_label_idx)] * 50)\n",
        "\n",
        "        return encoded_input, encoded_label\n",
        "\n",
        "train_dataset = MyDataset(train_texts, train_labels)\n",
        "valid_dataset = MyDataset(val_texts, val_labels)\n",
        "test_dataset = MyDataset(test_texts, test_labels)\n",
        "\n",
        "# Load model for token classification\n",
        "model = RobertaForTokenClassification.from_pretrained(\"uhhlt/am-roberta\", num_labels=train_dataset.num_labels)\n",
        "\n",
        "for param in model.roberta.parameters():\n",
        "    param.requires_grad = False  # Freeze the BERT layers\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Prepare DataLoader\n",
        "def collate_fn(batch):\n",
        "    if not batch:\n",
        "        return None\n",
        "\n",
        "    data_batch, label_batch = zip(*batch)\n",
        "\n",
        "    padded_input_ids = torch.nn.utils.rnn.pad_sequence([enc['input_ids'] for enc in data_batch],\n",
        "                                                       batch_first=True,\n",
        "                                                       padding_value=tokenizer.pad_token_id)\n",
        "    padded_attention_mask = torch.nn.utils.rnn.pad_sequence([enc['attention_mask'] for enc in data_batch],\n",
        "                                                            batch_first=True)\n",
        "\n",
        "    # Padding the labels\n",
        "    if label_batch and isinstance(label_batch[0], torch.Tensor):\n",
        "        # If the labels are already tensors, just pad them\n",
        "        padded_label_batch = torch.nn.utils.rnn.pad_sequence(label_batch, batch_first=True, padding_value=train_dataset.unknown_label_idx)\n",
        "    elif label_batch:\n",
        "        # If the labels are lists, convert them to tensors and pad them\n",
        "        max_label_len = max([len(labels) for labels in label_batch])\n",
        "        padded_label_batch = torch.tensor([labels + [train_dataset.unknown_label_idx] * (max_label_len - len(labels)) for labels in label_batch])\n",
        "    else:\n",
        "        padded_label_batch = []\n",
        "\n",
        "    return padded_input_ids, padded_attention_mask, padded_label_batch\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=4, pin_memory=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_batches = 0\n",
        "    for batch_input_ids, batch_attention_mask, batch_label_batch in train_loader:\n",
        "        batch_input_ids = batch_input_ids.squeeze(1)\n",
        "        batch_attention_mask = batch_attention_mask.squeeze(1)\n",
        "        batch_label_batch = batch_label_batch\n",
        "\n",
        "        batch_input_ids = batch_input_ids.to(device)\n",
        "        batch_attention_mask = batch_attention_mask.to(device)\n",
        "        batch_label_batch = batch_label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_label_batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Printing the progress\n",
        "        if (num_batches + 1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{num_batches}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    average_train_loss = total_train_loss / num_batches\n",
        "    print(f\"Epoch {epoch + 1} - Average training loss: {average_train_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    num_valid_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for val_input_ids, val_attention_mask, val_label_batch in valid_loader:\n",
        "            val_input_ids = val_input_ids.squeeze(1)\n",
        "            val_attention_mask = val_attention_mask.squeeze(1)\n",
        "            val_label_batch = val_label_batch\n",
        "\n",
        "            val_input_ids = val_input_ids.to(device)\n",
        "            val_attention_mask = val_attention_mask.to(device)\n",
        "            val_label_batch = val_label_batch.to(device)\n",
        "\n",
        "            outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask, labels=val_label_batch)\n",
        "            loss = outputs.loss\n",
        "            total_valid_loss += loss.item()\n",
        "            num_valid_batches += 1\n",
        "\n",
        "    average_valid_loss = total_valid_loss / num_valid_batches\n",
        "    print(f\"Epoch {epoch + 1} - Average validation loss: {average_valid_loss:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if average_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = average_valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Inference function\n",
        "def predict(test_loader, model, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for test_input_ids, test_attention_mask, test_label_batch in test_loader:\n",
        "            test_input_ids = test_input_ids.squeeze(1)\n",
        "            test_attention_mask = test_attention_mask.squeeze(1)\n",
        "            test_label_batch = test_label_batch\n",
        "\n",
        "            test_input_ids = test_input_ids.to(device)\n",
        "            test_attention_mask = test_attention_mask.to(device)\n",
        "            test_label_batch = test_label_batch.to(device)\n",
        "\n",
        "            outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask, labels=test_label_batch)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "\n",
        "            # Decode the predictions back to labels\n",
        "            decoded_preds = [oe.inverse_transform(preds[i].reshape(-1, 1)) for i in range(len(preds))]\n",
        "            predictions.extend(decoded_preds)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Perform predictions\n",
        "test_predictions = predict(test_loader, model, device)\n",
        "\n",
        "# Post-process predictions\n",
        "predicted_labels = []\n",
        "for prediction in test_predictions:\n",
        "    predicted_labels.extend(prediction.flatten().tolist())\n",
        "\n",
        "print(predicted_labels[:10])  # Print first 10 predicted labels for inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "dyxs1EkDUQcz"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}